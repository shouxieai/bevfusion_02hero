{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注册前向钩子（forward前），Module.register_forward_pre_hook(hook) -> torch.utils.hooks.RemovableHandle\n",
    "* 返回类型是torch.utils.hooks.RemovableHandle，使用handle.remove()删掉该钩子\n",
    "* 调用模块forward前执行钩子!!!\n",
    "* 钩子函数格式是：hook(module, input) -> None or modified input，返回值为None或者返回修改后的input\n",
    "* 钩子编程是编程范式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -  假如你的模型有三个步骤才能走完。 你在第二个步骤后添加一个钩子。  模型运行时，会先运行前两个步骤，然后再执行钩子。再执行第三个步骤\n",
    "      - 钩子可以把一些当前的数据带出来。实现可视化等功能。\n",
    "      \n",
    " - return: 如果return了数据，return的数据将 改变input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==[Hook]==\t Pre Hook: Model(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      ") \n",
      "==[Hook]==\t Input shape is: torch.Size([1, 1, 5, 5])\n",
      "==[Hook]==\t Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "==[forward]==\t Do forward, x.shape is torch.Size([1, 1, 7, 7])\n",
      "2\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139743865377872 \t output_id:139743865378112\n",
      "3\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139743865378512 \t output_id:139743865378432\n",
      "==[forward]==\t End forward, output shape is torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "'''主要改变输入的信息。\n",
    "主要功能。在forward前修改输入\n",
    "    - 本来输出是 1,2,1,1\n",
    "    - 增加hook结果输出时 1,2,3,3\n",
    "'''\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 2, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"==[forward]==\\t Do forward, x.shape is\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        output = F.relu(self.conv2(x))\n",
    "        print(\"==[forward]==\\t End forward, output shape is\", output.shape)\n",
    "        return output\n",
    "    \n",
    "def before_hook2(module, x):\n",
    "    #input 是tuple\n",
    "    print(\"==[Hook]==\\t Pre Hook:\", module, \"\\n==[Hook]==\\t Input shape is:\", x[0].shape)\n",
    "    print(f\"==[Hook]==\\t \", module.conv1, sep=\"\")\n",
    "    return torch.zeros(1, 1, 7, 7) # 使得在forward前，输入数据已经从torch.zeros(1, 1, 5, 5)修改为torch.zeros(1, 1, 7, 7)\n",
    "\n",
    "model2 = Model()\n",
    "handle = model2.register_forward_pre_hook(before_hook2) # model2.后面注册，说明修改所有模型的前向。\n",
    "input1 = torch.zeros(1, 1, 5, 5) # 输入形状(1, 1, 5, 5) ，正常经过网络卷积，输出应该是(1, 2, 1, 1)\n",
    "model2(input1)\n",
    "handle.remove()    # 删除\n",
    "\n",
    "# 结论：register_forward_pre_hook钩子方便 模型的 修改输入数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注册前向钩子（forward后），Module.register_forward_hook(hook) -> torch.utils.hooks.RemovableHandle\n",
    "* 返回类型是torch.utils.hooks.RemovableHandle，使用handle.remove()删掉该钩子\n",
    "* 调用模块forward后执行钩子\n",
    "* 钩子函数格式是：hook(module, input, output) -> None or modified output，返回值为None或者返回修改后的output\n",
    "* 钩子编程是编程范式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==[forward]==\t Do forward, x.shape is torch.Size([1, 1, 5, 5])\n",
      "4\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139750341919184 \t output_id:139743865338032\n",
      "5\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139750341919984 \t output_id:139750341919904\n",
      "==[forward]==\t End forward, output shape is torch.Size([1, 2, 1, 1])\n",
      "==[Hook]==\t this is register_forward_hook Model3(\n",
      "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      ") \n",
      "==[Hook]==\t this is x\t torch.Size([1, 1, 5, 5]) \n",
      "==[Hook]==\t this is output\t torch.Size([1, 2, 1, 1])\n",
      "==[res]==\t torch.Size([1, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 2, 3)\n",
    "    def forward(self, x):\n",
    "        print(\"==[forward]==\\t Do forward, x.shape is\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(\"==[forward]==\\t End forward, output shape is\", x.shape)\n",
    "        return x\n",
    "\n",
    "def hook_function(module, x, output):\n",
    "    print(f\"==[Hook]==\\t this is register_forward_hook\", module, f\"\\n==[Hook]==\\t\", \"this is x\\t\", x[0].shape,f\"\\n==[Hook]==\\t\", \"this is output\\t\", output.shape)\n",
    "    # 本来，模型结果形状是  [1, 2, 1, 1]\n",
    "    return torch.zeros(1, 1, 7, 7) # 但是hook在forward执行完。最后，return的结果改了output\n",
    "    \n",
    "model3 = Model3()\n",
    "x = torch.randn((1, 1, 5, 5))\n",
    "handel = model3.register_forward_hook(hook_function)\n",
    "res = model3(x)\n",
    "print(f\"==[res]==\\t\", res.shape)\n",
    "handel.remove()\n",
    "\n",
    "# 结论：register_forward_hook钩子方便修改 模型的 输出数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 总结：\n",
    "    - model.register_forward_pre_hook。  forward前的钩子return的值改的是input\n",
    "    - model.register_forward_hook。      forward后的钩子return的值改的是output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接替换forward函数\n",
    "- 1. 时刻记得使用钩子函数的目的：使用onnx.helper。创建node、initilizer等，并得到输入, 输出，根据输入输出划线(形象的理解)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 nn层的forward替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==[nn.Conv2d.forward.__closure__]==\t\t\t (<cell at 0x7f1a38038df0: function object at 0x7f18a7737a60>,)\n",
      "==[nn.Conv2d.forward.__closure__[0].cell_contents]==\t <function hook_function3.<locals>.inner_function at 0x7f18a7737a60>\n",
      "==[forward]==\t Do forward, x.shape is torch.Size([1, 1, 5, 5])\n",
      "6\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139743865339792 \t output_id:139750341879904\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139743865339792 \t output_id:139750341879904\n",
      "7\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139750341934272 \t output_id:139743865378032\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:139750341934272 \t output_id:139743865378032\n",
      "==[forward]==\t End forward, output shape is torch.Size([1, 2, 1, 1])\n",
      "torch.Size([1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 2, 3)\n",
    "    def forward(self, x):\n",
    "        print(\"==[forward]==\\t Do forward, x.shape is\", x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print(\"==[forward]==\\t End forward, output shape is\", x.shape)\n",
    "        return x\n",
    "\n",
    "# 单独改torch.nn.Conv2d层的forward\n",
    "def hook_function3(oldfunc):\n",
    "    def inner_function(self, x): # 入参与原先的forward数量保持一致\n",
    "        res = oldfunc(self, x)\n",
    "        print(f\"==[Hook]==\\t type:{type(self)} \\t input_id:{id(x)} \\t output_id:{id(res)}\")\n",
    "        return res\n",
    "    return inner_function\n",
    "\n",
    "\n",
    "nn.Conv2d.forward = hook_function3(nn.Conv2d.forward) # 把nn.Conv2d.forward函数替换成了inner_function函数\n",
    "# 一般一个函数结束，函数内部的变量、参数都会被释放掉。\n",
    "# 而闭包特性不一样，根据闭包的特性，inner_function用到了hook_function3的入参--oldfunc，\n",
    "    # 所以oldfunc并不会被释放，而是保存在__closure__中。\n",
    "print(f\"==[nn.Conv2d.forward.__closure__]==\\t\\t\\t\", nn.Conv2d.forward.__closure__) \n",
    "print(f\"==[nn.Conv2d.forward.__closure__[0].cell_contents]==\\t\", nn.Conv2d.forward.__closure__[0].cell_contents) #闭包内的参数保存下来\n",
    "\n",
    "# def printrelu(*args, **kwargs):\n",
    "#     print(\"myrelu\")\n",
    "# nn.ReLU.forward = hook_function3(printrelu)  # F.relu 这种形式改不了\n",
    "\n",
    "model4 = Model3().eval()\n",
    "x = torch.randn((1, 1, 5, 5))\n",
    "output = model4(x)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n",
    "# 总结：替换了 特定nn层 的forward函数。\n",
    "# 方法：使用python特性，直接替换特定函数的实现，以实现自定义修改。\n",
    "# 这种替换更为灵活。没有上面两个的局限性。 另外，这种写法，在没有影响结果的情况下，多做了一些事情"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 在1.0的基础上，尝试添加创建onnx节点，了解会有哪些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:140192944507696 \t output_id:140192944508176\n",
      "1\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:140192944508176 \t output_id:140192944508576\n",
      "tensor([[[[ 0.4775]],\n",
      "\n",
      "         [[-0.1290]]]], grad_fn=<ThnnConv2DBackward0>)\n",
      "ir_version: 9\n",
      "opset_import {\n",
      "  domain: \"\"\n",
      "  version: 11\n",
      "}\n",
      "producer_name: \"pytorch\"\n",
      "producer_version: \"1.9\"\n",
      "graph {\n",
      "  node {\n",
      "    input: \"140192944507696\"\n",
      "    input: \"conv1.weight\"\n",
      "    input: \"conv1.bias\"\n",
      "    output: \"140192944508176\"\n",
      "    name: \"Conv1\"\n",
      "    op_type: \"Conv\"\n",
      "    attribute {\n",
      "      name: \"auto_pad\"\n",
      "      type: STRING\n",
      "      s: \"NOTSET\"\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"dilations\"\n",
      "      type: INTS\n",
      "      ints: 1\n",
      "      ints: 1\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"group\"\n",
      "      type: INT\n",
      "      i: 1\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"kernel_shape\"\n",
      "      type: INTS\n",
      "      ints: 3\n",
      "      ints: 3\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pads\"\n",
      "      type: INTS\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"strides\"\n",
      "      type: INTS\n",
      "      ints: 1\n",
      "      ints: 1\n",
      "    }\n",
      "  }\n",
      "  node {\n",
      "    input: \"140192944508176\"\n",
      "    input: \"conv2.weight\"\n",
      "    input: \"conv2.bias\"\n",
      "    output: \"140192944508576\"\n",
      "    name: \"Conv2\"\n",
      "    op_type: \"Conv\"\n",
      "    attribute {\n",
      "      name: \"auto_pad\"\n",
      "      type: STRING\n",
      "      s: \"NOTSET\"\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"dilations\"\n",
      "      type: INTS\n",
      "      ints: 1\n",
      "      ints: 1\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"group\"\n",
      "      type: INT\n",
      "      i: 1\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"kernel_shape\"\n",
      "      type: INTS\n",
      "      ints: 3\n",
      "      ints: 3\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"pads\"\n",
      "      type: INTS\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "      ints: 0\n",
      "    }\n",
      "    attribute {\n",
      "      name: \"strides\"\n",
      "      type: INTS\n",
      "      ints: 1\n",
      "      ints: 1\n",
      "    }\n",
      "  }\n",
      "  name: \"mymodel\"\n",
      "  initializer {\n",
      "    dims: 3\n",
      "    dims: 1\n",
      "    dims: 3\n",
      "    dims: 3\n",
      "    data_type: 1\n",
      "    name: \"conv1.weight\"\n",
      "    raw_data: \"\\243\\344\\004\\276\\366p >\\320I=\\276\\240r->\\253(m;.}\\t>K\\213\\207\\276k\\321\\230\\274\\020\\241\\301\\275@%\\271<+\\203\\317\\274V\\031\\346;\\020S\\215=\\350bs\\276\\200\\304w\\274X\\204X>\\200W\\262=;\\206\\r\\276\\020\\365\\310\\2758\\032\\030>V\\261\\021\\273\\243\\261->\\014\\316\\212\\276\\317\\026\\220>&\\224\\265\\275k\\351\\262\\274 +#\\276\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 3\n",
      "    data_type: 1\n",
      "    name: \"conv1.bias\"\n",
      "    raw_data: \"\\310dI\\276\\337\\213\\244\\276\\3033\\206\\276\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 2\n",
      "    dims: 3\n",
      "    dims: 3\n",
      "    dims: 3\n",
      "    data_type: 1\n",
      "    name: \"conv2.weight\"\n",
      "    raw_data: \"H\\377\\322=(\\346\\361<\\350\\244B\\276\\003\\\\]=j\\202\\347\\275h`.\\276p\\035\\247\\275\\217(\\246\\275W6D\\275,t\\036\\275\\321$-<\\\"\\312?=\\014\\334$\\276WVL<}\\346P=(\\220\\006>[v\\244=\\267\\224\\206\\275F\\024\\203<u=\\205\\2752\\341\\267\\275c\\307\\007>\\270`8=\\322\\034\\010\\275\\204\\252\\356=fq\\220\\275\\375\\273\\374<\\014\\357\\004<\\225\\254\\301=\\206\\027\\007=\\267\\247\\037>\\000q\\177\\275\\035\\270$>1o\\272=\\354:\\326\\275\\323|D\\276\\341\\004\\262\\275\\263\\225D<Na\\244\\275\\004\\020}\\275\\341\\222\\003=\\003\\332n\\274e\\303\\r\\276\\034\\336\\014>\\022\\347>\\276\\251cd\\275\\240\\230\\004\\276\\246]4>%\\252\\240\\27404\\366\\275\\337z\\n>e\\027\\007>7\\375\\253\\274\\206\\240\\222\\275\"\n",
      "  }\n",
      "  initializer {\n",
      "    dims: 2\n",
      "    data_type: 1\n",
      "    name: \"conv2.bias\"\n",
      "    raw_data: \"\\271\\361\\311=<\\366\\025\\275\"\n",
      "  }\n",
      "  input {\n",
      "    name: \"140192944507696\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 5\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 5\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  output {\n",
      "    name: \"140192944508576\"\n",
      "    type {\n",
      "      tensor_type {\n",
      "        elem_type: 1\n",
      "        shape {\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 2\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "          dim {\n",
      "            dim_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Done.!\n",
      "[input: \"140192944507696\"\n",
      "input: \"conv1.weight\"\n",
      "input: \"conv1.bias\"\n",
      "output: \"140192944508176\"\n",
      "name: \"Conv1\"\n",
      "op_type: \"Conv\"\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  type: STRING\n",
      "  s: \"NOTSET\"\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  type: INTS\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "}\n",
      "attribute {\n",
      "  name: \"group\"\n",
      "  type: INT\n",
      "  i: 1\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  type: INTS\n",
      "  ints: 3\n",
      "  ints: 3\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  type: INTS\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  type: INTS\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "}\n",
      ", input: \"140192944508176\"\n",
      "input: \"conv2.weight\"\n",
      "input: \"conv2.bias\"\n",
      "output: \"140192944508576\"\n",
      "name: \"Conv2\"\n",
      "op_type: \"Conv\"\n",
      "attribute {\n",
      "  name: \"auto_pad\"\n",
      "  type: STRING\n",
      "  s: \"NOTSET\"\n",
      "}\n",
      "attribute {\n",
      "  name: \"dilations\"\n",
      "  type: INTS\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "}\n",
      "attribute {\n",
      "  name: \"group\"\n",
      "  type: INT\n",
      "  i: 1\n",
      "}\n",
      "attribute {\n",
      "  name: \"kernel_shape\"\n",
      "  type: INTS\n",
      "  ints: 3\n",
      "  ints: 3\n",
      "}\n",
      "attribute {\n",
      "  name: \"pads\"\n",
      "  type: INTS\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "  ints: 0\n",
      "}\n",
      "attribute {\n",
      "  name: \"strides\"\n",
      "  type: INTS\n",
      "  ints: 1\n",
      "  ints: 1\n",
      "}\n",
      "]\n",
      "['140192944507696', '140192944508176']\n",
      "['140192944508176', '140192944508576']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# 新增的包\n",
    "import onnx\n",
    "import onnx.helper as helper\n",
    "import numpy as np\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 3, 3)\n",
    "        self.conv2 = nn.Conv2d(3, 2, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # F.relu的形式不行，这里为了演示，去掉F.relu\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "nodes = []\n",
    "initializers = [] \n",
    "global_input_name = []\n",
    "global_output_name = []\n",
    "# global_input = [] # 暂时不需要保存global_input\n",
    "# global_output = []\n",
    "\n",
    "# 写inputs时需要\n",
    "def append_initializer(value, name):\n",
    "    initializers.append(#https://onnx.ai/onnx/api/helper.html#onnx.helper.make_tensor # 所有的数据value 添加到initializers列表中\n",
    "        helper.make_tensor(\n",
    "            name=name,\n",
    "            data_type=helper.TensorProto.DataType.FLOAT, # 类型用FLOAT\n",
    "            dims=list(value.shape),\n",
    "            vals=value.cpu().data.numpy().astype(np.float32).tobytes(),\n",
    "            raw=True\n",
    "        )\n",
    "    )\n",
    "    return name    \n",
    "  \n",
    "def hook_function3(oldfunc, ilayer):\n",
    "    def inner_function(self, x):\n",
    "        # 新增层name计数\n",
    "        nonlocal ilayer\n",
    "        print(ilayer)\n",
    "        ilayer += 1\n",
    "        \n",
    "        # 老函数的前向结果\n",
    "        res = oldfunc(self, x) # 写inputs的时候，断点可以打在这里看conv.weight.data\n",
    "        global_output_name.append(str(id(res)))\n",
    "        # global_output.append(res)\n",
    "        \n",
    "        # 看内存地址打印信息\n",
    "        print(f\"==[Hook]==\\t type:{type(self)} \\t input_id:{id(x)} \\t output_id:{id(res)}\")    \n",
    "        \n",
    "        # inputs列表\n",
    "        global_input_name.append(str(id(x)))\n",
    "        # global_input.append(x)\n",
    "        \n",
    "        inputs = [\n",
    "            str(id(x)), # 用x的id的字符串作为名字\n",
    "            append_initializer(self.weight.data, f\"conv{ilayer}.weight\"), # scn中有permute这里weight的形状就是out in kernel kernel 所以不用permute\n",
    "        ]\n",
    "        if self.bias is not None: # 有 bias 的话，就会将 \"spconv0.bias\" 作为第三个元素\n",
    "            inputs.append(append_initializer(self.bias.data, f\"conv{ilayer}.bias\"))\n",
    "\n",
    "        # 添加onnx的node节点\n",
    "        nodes.append(\n",
    "            helper.make_node(\n",
    "            name=f\"Conv{ilayer}\", # 使用ilayer区分名字\n",
    "            op_type=\"Conv\",\n",
    "            inputs=inputs,\n",
    "            outputs=[str(id(res))],  \n",
    "            auto_pad=\"NOTSET\",\n",
    "            pads=[0, 0, 0, 0],\n",
    "            group=1,\n",
    "            dilations=[1, 1],\n",
    "            kernel_shape=[3, 3],\n",
    "            strides=[1, 1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return res\n",
    "    return inner_function\n",
    "\n",
    "ilayer = 0 # 新增变量，区分创建的onnx的名字\n",
    "nn.Conv2d.forward = hook_function3(nn.Conv2d.forward, ilayer) # 把nn.Conv2d.forward函数替换成了inner_function函数\n",
    "\n",
    "\n",
    "model4 = Model3().eval()\n",
    "x = torch.randn((1, 1, 5, 5))\n",
    "output = model4(x)\n",
    "print(output)\n",
    "\n",
    "inputs = [\n",
    "    helper.make_value_info( #创建ValueInfoProto\n",
    "        name=global_input_name[0],\n",
    "        type_proto=helper.make_tensor_type_proto(\n",
    "            elem_type=helper.TensorProto.DataType.FLOAT,\n",
    "            shape=[1, 1, 5, 5]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    helper.make_value_info(\n",
    "        name=global_output_name[-1],\n",
    "        type_proto=helper.make_tensor_type_proto(\n",
    "            elem_type=helper.TensorProto.DataType.FLOAT,\n",
    "            shape=[1, 2, 1, 1]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "graph = helper.make_graph(\n",
    "    name=\"mymodel\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    nodes=nodes,\n",
    "    initializer=initializers\n",
    ")\n",
    "\n",
    "# 如果名字不是ai.onnx，netron解析就不是太一样了\n",
    "opset = [\n",
    "    helper.make_operatorsetid(\"\", 11)\n",
    "]\n",
    "\n",
    "# producer主要是保持和pytorch一致\n",
    "model = helper.make_model(graph, opset_imports=opset, producer_name=\"pytorch\", producer_version=\"1.9\")\n",
    "\n",
    "onnx.checker.check_model(model)\n",
    "onnx.save_model(model, \"trace.onnx\")\n",
    "\n",
    "print(model)\n",
    "print(\"Done.!\")\n",
    "\n",
    "print(nodes)\n",
    "print(global_input_name)\n",
    "print(global_output_name)\n",
    "\n",
    "# 总结：本案例主要确定创建onnx的思想。但是仍有问题。暂且不表\n",
    "# 下面我们主要完善钩子函数，暂时不写onnx的逻辑\n",
    "\n",
    "\n",
    "# 起始肉眼可见的缺点就有，名字太长。\n",
    "    #　卷积这里只有两个，多增加卷积。就有可能涉及内存的问题\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 1, 1)\n",
      "[[[[ 0.47749427]]\n",
      "\n",
      "  [[-0.12904355]]]]\n"
     ]
    }
   ],
   "source": [
    "# 模型推理\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 准备输入数据\n",
    "x = x.numpy()\n",
    "input_val = np.array(x, dtype=np.float32)\n",
    "\n",
    "# 加载模型并指定执行提供者\n",
    "sess = onnxruntime.InferenceSession(\"trace.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "# 获取模型的输入名称\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "# 运行模型\n",
    "output = sess.run(None, {input_name: input_val})\n",
    "\n",
    "print(output[0].shape)\n",
    "print(output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 relu也替换掉的例子\n",
    "- 这里换了一个conv与relu分开的网络\n",
    "- 引出内存id的复用问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myconv\n",
      "myrelu\n",
      "myconv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Model5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(3, 1, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "        \n",
    "# 1.0演示直接替换    \n",
    "# def printconv(*args, **kwargs):\n",
    "#     print(\"myconv\")\n",
    "# def printrelu(*args, **kwargs):\n",
    "#     print(\"myrelu\")\n",
    "    \n",
    "# nn.Conv2d.forward = printconv   \n",
    "# nn.ReLU.forward = printrelu  \n",
    "  \n",
    "# 2.0 正式代码\n",
    "def hook_function(oldfunc): # 传入的参数是一个函数地址，即被替换掉的函数。\n",
    "    def inner_function(self, x): # 内部的函数参数形式要与forward一样。\n",
    "        res = oldfunc(self, x)\n",
    "        print(f\"==[Hook]==\\t {type(self)}, input id = {id(x)}, output id = {id(res)}\")\n",
    "        return res\n",
    "    return inner_function\n",
    "    \n",
    "nn.Conv2d.forward = hook_function(nn.Conv2d.forward) # 替换所有Conv2d模块的前向函数\n",
    "nn.ReLU.forward =   hook_function(nn.ReLU.forward)   \n",
    "\n",
    "model = Model5().eval()\n",
    "input = torch.zeros(1, 3, 3, 3)\n",
    "a = model(input)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "==[Hook]==\t <class 'torch.nn.modules.conv.Conv2d'>, input id = 139901407312928, output id = 139901407312448\n",
    "==[Hook]==\t <class 'torch.nn.modules.activation.ReLU'>, input id = 139901407312448, output id = 139901407310768\n",
    "==[Hook]==\t <class 'torch.nn.modules.conv.Conv2d'>, input id = 139901407310768, output id = 139901407312448\n",
    "\n",
    "id = 139901407312448 出现了多次。这是因为pytorch的内存复用造成的结果。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 更新版\n",
    "- 对程序进一步修改。\n",
    "- 主要功能仍然是，将nn模块自带的forward函数，在功能(前向)不变的情况下，替换为自定义myforward函数。\n",
    "- 使用带参装饰器，参数为想要改变的 nn模块 + forwar字符串。\n",
    "  - 例如像改变  Conv2d 模块 的 forward。 传参 \"torch.nn.Conv2d.forward\"\n",
    "- 使用getattr，在函数内得到与装饰器参数一致的forward对象\n",
    "- 使用setattr，真正做到修改forward函数\n",
    "- 使用clone()，解决pytorch自动优化tensor导致tensor复用的问题。仍然留下一个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为什么要避免复用？\n",
    "# 主要和后面构建onnx有关。\n",
    "# ==[Hook]==\t <class 'torch.nn.modules.conv.Conv2d'>, input id = 139901407312928, output id = 139901407312448\n",
    "# ==[Hook]==\t <class 'torch.nn.modules.activation.ReLU'>, input id = 139901407312448, output id = 139901407310768\n",
    "# ==[Hook]==\t <class 'torch.nn.modules.conv.Conv2d'>, input id = 139901407310768, output id = 139901407312448\n",
    "# 从打印结果看，第一个Conv2d的output的id与第二个Conv2d的output的id一致。这样在构建graph时会出现问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>, input id = 139735143086256, output id = 139735143200224\n",
      "<class 'torch.nn.modules.activation.ReLU'>, input id = 139735143200224, output id = 139735143200704\n",
      "<class 'torch.nn.modules.conv.Conv2d'>, input id = 139735143200704, output id = 139735143200464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.1670, -0.1670, -0.1670],\n",
       "          [-0.1670, -0.1670, -0.1670],\n",
       "          [-0.1670, -0.1670, -0.1670]]]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(3, 1, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "        \n",
    "def hook_forward(fn):# 再包裹一层，使得能够使用装饰器。\n",
    "    fnname_list = fn.split(\".\") # ['torch', 'nn', 'Conv2d', 'forward']\n",
    "    layer_name = eval(\".\".join(fnname_list[:-1])) # 得到torch.nn.Conv2d\n",
    "    fn_name = fnname_list[-1] # \"forward\"\n",
    "    oldfn = getattr(layer_name, fn_name) # 得到<function Conv2d.forward at 0x000002AAED0B8940>\n",
    "                                         # <function ReLU.forward at 0x000002AAED0D31F0>\n",
    "    \n",
    "    def make_hook(bind_func):\n",
    "        \n",
    "        def myforward(self, x):\n",
    "            y = oldfn(self, x).clone() # 避免pytorch对tensor复用，但并不能总是保证id唯一。为何要避免复用，见最后面？\n",
    "            bind_func(self, x, y) \n",
    "            return y\n",
    "        \n",
    "        setattr(layer_name, fn_name, myforward) # 改变forward\n",
    "        \n",
    "        return myforward\n",
    "    \n",
    "    return make_hook\n",
    "    \n",
    "\n",
    "@hook_forward(\"torch.nn.Conv2d.forward\")\n",
    "def symbolic_conv2d(self, x, y):\n",
    "    print(f\"{type(self)}, input id = {id(x)}, output id = {id(y)}\")\n",
    "    \n",
    "@hook_forward(\"torch.nn.ReLU.forward\")\n",
    "def symbolic_relu(self, x, y):\n",
    "    print(f\"{type(self)}, input id = {id(x)}, output id = {id(y)}\")\n",
    "    \n",
    "model = Model().eval()\n",
    "input1 = torch.ones(1, 3, 3, 3)\n",
    "a = model(input1)\n",
    "a\n",
    "\n",
    "\n",
    "# layer_name有哪些键\n",
    "# for k,v in vars(layer_name).items():\n",
    "#     print(k)\n",
    "# __module__\n",
    "# __doc__\n",
    "# __init__\n",
    "# _conv_forward\n",
    "# forward\n",
    "\n",
    "# id有什么变化\n",
    "# 除特殊情况，id已经没有重复的了。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "执行顺序说明：\n",
    "1.0 代码从上往下执行, 先导入各种模块, 定义Model类，定义hook_forward函数\n",
    "\n",
    "2.0 再执行到带参数的@hook_forward装饰器。执行hook_forward前4行代码\n",
    "    定义make_hook函数，返回make_hook的引用\n",
    "\n",
    "3.0 定义symbolic_conv2d函数，当被装饰的函数symbolic_conv2d定义好了\n",
    "    则将被装饰的函数作为参传入刚刚执行装饰器返回的myforward函数并执行, \n",
    "    即执行myforward(symbolic_conv2d)\n",
    "    \n",
    "4.0 再定义myforward函数, 利用setattr，用myforward替换掉\n",
    "    <class 'torch.nn.modules.conv.Conv2d'>的forward属性，\n",
    "\n",
    "ReLU的执行重复3.0  4.0 即\n",
    "5.0 再执行到带参数的@hook_forward装饰器。执行hook_forward前4行代码\n",
    "    定义make_hook函数，返回make_hook的引用\n",
    "\n",
    "6.0 再定义myforward函数, 利用setattr，用myforward替换掉\n",
    "    <class 'torch.nn.modules.activation.ReLU'>的forward属性\n",
    "\n",
    "7.0 实例化Model，执行__init__，置为eval模式，构造数据，执行model的前向forward\n",
    "\n",
    "8.0 在执行self.conv1(x)时，实际执行的是hook_forward中的make_hook中的myforward\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 再改进版\n",
    "- tensor什么时候会复用？\n",
    "    - 答案：在tensor没有任何引用的时候，tensor会被回收，并且会被复用\n",
    "    - 举例：在forward函数中\n",
    "    \n",
    "    ![Alt text](./image.png)\n",
    "    \n",
    "        - 执行完self.conv1(x)后，x1的引用计数减1，此时引用计数不一定为0(外面可能会有别的引用)，x1不一定会被回收\n",
    "        - 执行完self.relu(x)后, x2是真正的没有引用了。此时就可能被复用\n",
    "        - 在执行self.conv2(x)时。等号左边的x6需要新的内存，\n",
    "            - 此时发现x2没有引用了，内存还没被释放\n",
    "            - 于是就会使用这个内存。\n",
    "\n",
    "- 如何不让他复用？\n",
    "    - 解决方案是，使其引用计数一直存在，不释放。\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 目标1：彻底解决tensor复用问题\n",
    "- 方法：创建全局变量  列表 all_tensors, 将新myforward函数的产生的所有输入输出，都存起来。始终不让tensor引用计数器变为0\n",
    "- 新增功能：把print打印的地址id，转换成从1开始的数字。主要是为了方便观看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:140192944592352 \t output_id:140192369543104\n",
      "<class 'torch.nn.modules.conv.Conv2d'>, input id = 0, output id = 1\n",
      "<class 'torch.nn.modules.activation.ReLU'>, input id = 1, output id = 2\n",
      "3\n",
      "==[Hook]==\t type:<class 'torch.nn.modules.conv.Conv2d'> \t input_id:140192369543504 \t output_id:140192369543424\n",
      "<class 'torch.nn.modules.conv.Conv2d'>, input id = 2, output id = 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n<class 'torch.nn.modules.conv.Conv2d'>, input id = 0, output id = 1\\n<class 'torch.nn.modules.activation.ReLU'>, input id = 1, output id = 2\\n<class 'torch.nn.modules.conv.Conv2d'>, input id = 2, output id = 3\\n[3]:\\ntensor([[[[-1.1552, -1.1552, -1.1552],\\n          [-1.1552, -1.1552, -1.1552],\\n          [-1.1552, -1.1552, -1.1552]]]], grad_fn=<ConvolutionBackward0>)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(3, 1, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "        \n",
    "def hook_forward(fn):\n",
    "    nnmodule_list = fn.split(\".\")\n",
    "    nnmodule_name = eval(\".\".join(nnmodule_list[:-1])) \n",
    "    func_name = nnmodule_list[-1]\n",
    "    oldfunc = getattr(nnmodule_name, func_name)\n",
    "    \n",
    "    def make_hook(bind_func):\n",
    "        def newforward(self, x):\n",
    "            global all_tensors\n",
    "            y = oldfunc(self, x)\n",
    "            all_tensors.extend([x, y])\n",
    "            bind_func(self, x, y)\n",
    "            return y\n",
    "        setattr(nnmodule_name, func_name, newforward)\n",
    "    return make_hook\n",
    "\n",
    "def get_obj_id(obj):\n",
    "    global objmap # 引入global变量objmap\n",
    "    obj_id = id(obj) # 用id作为键，用长度作为值。\n",
    "    \n",
    "    if obj_id not in objmap:\n",
    "        objmap[obj_id] = len(objmap)\n",
    "    return objmap[obj_id]\n",
    "        \n",
    "    \n",
    "@hook_forward(\"torch.nn.Conv2d.forward\")\n",
    "def symbolic_conv2d(self, x, y):\n",
    "    print(f\"{type(self)}, input id = {get_obj_id(x)}, output id = {get_obj_id(y)}\")\n",
    "    \n",
    "@hook_forward(\"torch.nn.ReLU.forward\")\n",
    "def symbolic_relu(self, x, y):\n",
    "    print(f\"{type(self)}, input id = {get_obj_id(x)}, output id = {get_obj_id(y)}\")\n",
    "    \n",
    "all_tensors = []\n",
    "objmap = {} \n",
    "    \n",
    "model = Model().eval()\n",
    "input1 = torch.ones(1, 3, 3, 3)\n",
    "a = model(input1)\n",
    "a\n",
    "\n",
    "\n",
    "'''\n",
    "<class 'torch.nn.modules.conv.Conv2d'>, input id = 0, output id = 1\n",
    "<class 'torch.nn.modules.activation.ReLU'>, input id = 1, output id = 2\n",
    "<class 'torch.nn.modules.conv.Conv2d'>, input id = 2, output id = 3\n",
    "[3]:\n",
    "tensor([[[[-1.1552, -1.1552, -1.1552],\n",
    "          [-1.1552, -1.1552, -1.1552],\n",
    "          [-1.1552, -1.1552, -1.1552]]]], grad_fn=<ConvolutionBackward0>)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导出onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnx\n",
    "import onnx.helper as helper\n",
    "import numpy as np\n",
    "\n",
    "# reference\n",
    "# https://github.com/shouxieai/learning-cuda-trt/blob/main/tensorrt-basic-1.4-onnx-editor/create-onnx.py\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 3, 1, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(3, 1, 1, 1) \n",
    "        self.conv_right = nn.Conv2d(3, 3, 1, 1) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        r = self.conv_right(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x + r) # 与上一个案例不同。这里增加了一个分支。目的是让onnx看起来复杂一点。不重要\n",
    "        return x\n",
    "\n",
    "def hook_forward(fn):\n",
    "    # torch.Tensor.__add__\n",
    "    fnnames   = fn.split(\".\")\n",
    "    fn_module = eval(\".\".join(fnnames[:-1])) # torch.Tensor\n",
    "    fn_name   = fnnames[-1] # __add__\n",
    "    oldfn = getattr(fn_module, fn_name) # getattr(torch.Tensor,  \"__add__\")\n",
    "    \n",
    "    def make_hook(bind_fn):\n",
    "\n",
    "        ilayer = 0  # 与上一个案例不同 ， 新增统计层数的变量\n",
    "        def myforward(self, x):\n",
    "            global all_tensors\n",
    "            nonlocal ilayer\n",
    "            y = oldfn(self, x)\n",
    "\n",
    "            bind_fn(self, ilayer, x, y)\n",
    "            all_tensors.extend([x, y])   # 避免torch对tensor进行复用\n",
    "            ilayer += 1\n",
    "            return y\n",
    "\n",
    "        setattr(fn_module, fn_name, myforward)\n",
    "    return make_hook\n",
    "\n",
    "@hook_forward(\"torch.nn.Conv2d.forward\")\n",
    "def symbolic_conv2d(self, ilayer, x, y):\n",
    "    # get_obj_idd方法，用字典的长度代表序号。序号用于追踪每个tensor\n",
    "    print(f\"{type(self)} -> Input {get_obj_idd(x)}, Output {get_obj_idd(y)}\")\n",
    "    \n",
    "    inputs = [ \n",
    "        get_obj_idd(x), # 层的输入x 对应的编号\n",
    "        append_initializer(self.weight.data, f\"conv{ilayer}.weight\"), \n",
    "        append_initializer(self.bias.data, f\"conv{ilayer}.bias\")\n",
    "    ]\n",
    "\n",
    "    nodes.append(\n",
    "        helper.make_node( # 真正创建onnx节点\n",
    "            \"Conv\", inputs, [get_obj_idd(y)], f\"conv{ilayer}\",   \n",
    "            kernel_shape=self.kernel_size, group=self.groups, pads=[0, 0] + list(self.padding), dilations=self.dilation, strides=self.stride\n",
    "            # op_type = \"Conv\"  ， inputs = inputs ， outputs = [get_obj_idd(y)]， name = f\"conv{ilayer}\"\n",
    "            # pads=[0, 0] + list(self.padding) 是因为这里的pads 需要格式是列表中中4个数   [0, 0， 0， 0]\n",
    "        )\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "@hook_forward(\"torch.nn.ReLU.forward\") # 将torch.nn.ReLU.forward 方法，替换为  myforward\n",
    "def symbolic_relu(self, ilayer, x, y):\n",
    "    print(f\"{type(self)} -> Input {get_obj_idd(x)}, Output {get_obj_idd(y)}\")\n",
    "\n",
    "    nodes.append(\n",
    "        helper.make_node(\n",
    "            \"Relu\", [get_obj_idd(x)], [get_obj_idd(y)], f\"relu{ilayer}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@hook_forward(\"torch.Tensor.__add__\")\n",
    "def symbolic_add(a, ilayer, b, y): \n",
    "    # 新增一个add，因为网络中涉及x = self.conv2(x + r)， 存在 x + r。 \n",
    "     # + 加号  在 torch.Tensor中，对python的魔术方法 __add__重写了。\n",
    "     # 所以，我们也需要把 torch.Tensor.__add__ 方法   替换为  我们自己的  add方法。\n",
    "     # 将torch.Tensor.__add__ 方法，替换为  myforward\n",
    "    print(f\"Add -> Input {get_obj_idd(a)} + {get_obj_idd(b)}, Output {get_obj_idd(y)}\")\n",
    "\n",
    "    nodes.append(\n",
    "        helper.make_node(\n",
    "            \"Add\", [get_obj_idd(a), get_obj_idd(b)], [get_obj_idd(y)], f\"add{ilayer}\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def append_initializer(value, name): # weight、bais在onnx中储存在initializer层。\n",
    "    initializers.append(\n",
    "        helper.make_tensor(\n",
    "            name=name, # 以卷积为例一般为：\"conv.weight\"或\"conv.bias\"\n",
    "            data_type=helper.TensorProto.DataType.FLOAT,\n",
    "            dims=list(value.shape),\n",
    "            vals=value.data.numpy().astype(np.float32).tobytes(),\n",
    "            raw=True\n",
    "        )\n",
    "    )\n",
    "    return name\n",
    "\n",
    "\n",
    "def get_obj_idd(obj):\n",
    "    global objmap\n",
    "\n",
    "    idd = id(obj)\n",
    "    if idd not in objmap:\n",
    "        objmap[idd] = str(len(objmap))\n",
    "    return objmap[idd]\n",
    "\n",
    "all_tensors = []\n",
    "objmap = {}\n",
    "nodes = []\n",
    "initializers = []\n",
    "\n",
    "torch.manual_seed(31)\n",
    "x = torch.full((1, 3, 3, 3), 0.55)\n",
    "model = Model().eval()\n",
    "y = model(x)\n",
    "\n",
    "inputs = [\n",
    "    helper.make_value_info(\n",
    "        name=\"0\",\n",
    "        type_proto=helper.make_tensor_type_proto(\n",
    "            elem_type=helper.TensorProto.DataType.FLOAT,\n",
    "            shape=[\"batch\", x.size(1), x.size(2), x.size(3)] \n",
    "            # batch用字符串，或-1，表示动态\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    helper.make_value_info(\n",
    "        name=\"5\",\n",
    "        type_proto=helper.make_tensor_type_proto(\n",
    "            elem_type=helper.TensorProto.DataType.FLOAT,\n",
    "            shape=[\"batch\", y.size(1), y.size(2), y.size(3)]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "graph = helper.make_graph(\n",
    "    name=\"mymodel\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    nodes=nodes,\n",
    "    initializer=initializers\n",
    ")\n",
    "\n",
    "# 如果名字不是ai.onnx，netron解析就不是太一样了\n",
    "opset = [\n",
    "    helper.make_operatorsetid(\"ai.onnx\", 11)\n",
    "]\n",
    "\n",
    "# !!!!!!!!!producer主要是保持和pytorch一致\n",
    "model = helper.make_model(graph, opset_imports=opset, producer_name=\"pytorch\", producer_version=\"1.9\")\n",
    "onnx.save_model(model, \"custom.onnx\")\n",
    "\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
